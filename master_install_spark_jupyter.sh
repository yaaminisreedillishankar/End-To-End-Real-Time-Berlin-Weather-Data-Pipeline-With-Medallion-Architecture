#!/usr/bin/env bash
# ==============================================================================
# Master installer: Apache Spark on YARN + Jupyter
# - Installs Spark (default 3.5.7, Hadoop 3 build) under /opt and /opt/spark symlink
# - Pins PySpark to Python 3.12 across driver & executors (fallbacks to /usr/bin/python3)
# - Writes /etc/profile.d/spark.sh with *fixed* Hadoop defaults:
#     HADOOP_HOME=/usr/local/hadoop
#     HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
# - Generates Spark conf for YARN, creates HDFS /spark-logs
# - Creates a JupyterLab venv and a ready-to-use kernel + launcher
# - Optionally systemd unit for JupyterLab
# ==============================================================================
set -euo pipefail

# === Tunables ================================================================
SPARK_VERSION="${SPARK_VERSION:-3.5.7}"
HADOOP_PROFILE="${HADOOP_PROFILE:-hadoop3}"
INSTALL_DIR="${INSTALL_DIR:-/opt}"
SPARK_SYMLINK="${SPARK_SYMLINK:-/opt/spark}"

# Use same interpreter on all nodes for executors
PYSPARK_PYTHON="${PYSPARK_PYTHON:-}"
if [[ -z "${PYSPARK_PYTHON}" ]]; then
  if [[ -x /usr/bin/python3.12 ]]; then PYSPARK_PYTHON=/usr/bin/python3.12;
  elif [[ -x /usr/local/bin/python3.12 ]]; then PYSPARK_PYTHON=/usr/local/bin/python3.12;
  else PYSPARK_PYTHON=$(command -v python3 || true);
  fi
fi
if [[ -z "${PYSPARK_PYTHON}" ]]; then
  echo "ERROR: Could not find a Python interpreter." >&2; exit 1
fi

# Jupyter settings
JUPYTER_BASE_DIR="${JUPYTER_BASE_DIR:-${SPARK_SYMLINK}}"
JUPYTER_VENV_DIR="${JUPYTER_VENV_DIR:-${JUPYTER_BASE_DIR}/venv}"
JUPYTER_BIND_IP="${JUPYTER_BIND_IP:-0.0.0.0}"
JUPYTER_PORT="${JUPYTER_PORT:-8888}"
JUPYTER_SYSTEMD="${JUPYTER_SYSTEMD:-false}"

CREATE_HDFS_EVENTLOG_DIR="${CREATE_HDFS_EVENTLOG_DIR:-true}"

WORKERS_STR="${WORKERS:-}"

# Fixed Hadoop defaults per request
HADOOP_HOME="/usr/local/hadoop"
HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop"

require_root() { if [[ $EUID -ne 0 ]]; then echo "Please run as root (sudo)." >&2; exit 1; fi; }

install_prereqs() {
  apt-get update -y
  DEBIAN_FRONTEND=noninteractive apt-get install -y \
    curl tar rsync openssh-client \
    python3 python3-venv
  if ! command -v java >/dev/null 2>&1; then
    DEBIAN_FRONTEND=noninteractive apt-get install -y openjdk-11-jdk
  fi
}

detect_java_home() {
  if command -v java >/dev/null 2>&1; then
    local jbin; jbin="$(readlink -f "$(command -v java)")"
    JAVA_HOME="$(dirname "$(dirname "$jbin")")"
  else JAVA_HOME=""; fi
}

download_spark() {
  local tgz="spark-${SPARK_VERSION}-bin-${HADOOP_PROFILE}.tgz"
  local url1="https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/${tgz}"
  local url2="https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/${tgz}"
  echo "Downloading Spark ${SPARK_VERSION} (${HADOOP_PROFILE}) if needed..."
  if [[ ! -f "/tmp/${tgz}" ]]; then
    set +e
    curl -fSL -o "/tmp/${tgz}" "$url1" || curl -fSL -o "/tmp/${tgz}" "$url2"
    local rc=$?; set -e
    if [[ $rc -ne 0 ]]; then echo "ERROR: Failed to download Spark tarball." >&2; exit 1; fi
  fi
  echo "Extracting into ${INSTALL_DIR}..."
  mkdir -p "${INSTALL_DIR}"
  tar -xzf "/tmp/${tgz}" -C "${INSTALL_DIR}"
  local extracted="${INSTALL_DIR}/spark-${SPARK_VERSION}-bin-${HADOOP_PROFILE}"
  ln -sfn "${extracted}" "${SPARK_SYMLINK}"
  chown -R root:root "${extracted}"
}

write_profile() {
  cat >/etc/profile.d/spark.sh <<EOF
# Added by master_install_spark_jupyter.sh
export SPARK_HOME="${SPARK_SYMLINK}"
export PATH="\$SPARK_HOME/bin:\$SPARK_HOME/sbin:\$PATH"
export JAVA_HOME="${JAVA_HOME}"
# Fixed Hadoop defaults
export HADOOP_HOME="/usr/local/hadoop"
export HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop"
# Executors' Python (must exist on all nodes)
export PYSPARK_PYTHON="${PYSPARK_PYTHON}"
EOF
  chmod 0644 /etc/profile.d/spark.sh
}

configure_spark() {
  local conf_dir="${SPARK_SYMLINK}/conf"; mkdir -p "${conf_dir}"
  # spark-env.sh
  cat >"${conf_dir}/spark-env.sh" <<'EOF'
#!/usr/bin/env bash
# Auto-generated by master_install_spark_jupyter.sh
export SPARK_HOME="{{SPARK_HOME}}"
export JAVA_HOME="{{JAVA_HOME}}"
export HADOOP_HOME="/usr/local/hadoop"
export HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop"
export PYSPARK_PYTHON="{{PYSPARK_PYTHON}}"
if [[ -x "/usr/local/hadoop/bin/hadoop" ]]; then
  export SPARK_DIST_CLASSPATH="$('/usr/local/hadoop/bin/hadoop' classpath)"
fi
EOF
  sed -i "s|{{SPARK_HOME}}|${SPARK_SYMLINK}|g" "${conf_dir}/spark-env.sh"
  sed -i "s|{{JAVA_HOME}}|${JAVA_HOME}|g" "${conf_dir}/spark-env.sh"
  sed -i "s|{{PYSPARK_PYTHON}}|${PYSPARK_PYTHON}|g" "${conf_dir}/spark-env.sh"
  chmod +x "${conf_dir}/spark-env.sh"

  # spark-defaults.conf
  cat >"${conf_dir}/spark-defaults.conf" <<EOF
# Auto-generated for YARN
spark.master                     yarn
spark.submit.deployMode          client
# Resources (tune)
spark.driver.memory              2g
spark.executor.memory            2g
spark.executor.cores             2
# History Server event logs
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs:///spark-logs
spark.history.fs.logDirectory    hdfs:///spark-logs
# Pin Python on driver & executors
spark.yarn.appMasterEnv.PYSPARK_PYTHON    ${PYSPARK_PYTHON}
spark.executorEnv.PYSPARK_PYTHON         ${PYSPARK_PYTHON}
# Route around flaky DNs (optional safety)
spark.hadoop.dfs.client.block.write.replace-datanode-on-failure.enable  true
spark.hadoop.dfs.client.block.write.replace-datanode-on-failure.policy  ALWAYS
spark.hadoop.dfs.client.writer.exclude.nodes.cache.expiry.interval.millis 60000
EOF

  : >"${conf_dir}/workers"  # not used by YARN
}

maybe_create_hdfs_dirs() {
  if [[ "${CREATE_HDFS_EVENTLOG_DIR}" == "true" ]] && command -v hdfs >/dev/null 2>&1; then
    echo "Creating HDFS /spark-logs (if missing)..."
    set +e
    hdfs dfs -mkdir -p /spark-logs
    hdfs dfs -chmod -R 1777 /spark-logs
    set -e
  fi
}

setup_jupyter_env() {
  echo "Setting up JupyterLab venv + kernel..."
  mkdir -p "${JUPYTER_BASE_DIR}"
  if [[ ! -d "${JUPYTER_VENV_DIR}" ]]; then
    "${PYSPARK_PYTHON}" -m venv "${JUPYTER_VENV_DIR}"
  fi
  # shellcheck disable=SC1090
  source "${JUPYTER_VENV_DIR}/bin/activate"
  pip install --upgrade pip
  pip install jupyterlab ipykernel findspark
  # Optional packages that are often needed
  pip install -q pandas pyarrow || true
  # Register kernel
  local kernels_dir="/usr/local/share/jupyter/kernels/pyspark_yarn"
  mkdir -p "${kernels_dir}"
  local py4j_zip
  py4j_zip=$(ls -1 "${SPARK_SYMLINK}/python/lib"/py4j-*.zip | head -n1)
  cat >"${kernels_dir}/kernel.json" <<EOF
{
  "argv": [
    "${JUPYTER_VENV_DIR}/bin/python",
    "-m", "ipykernel",
    "-f", "{connection_file}"
  ],
  "display_name": "PySpark (YARN)",
  "language": "python",
  "env": {
    "SPARK_HOME": "${SPARK_SYMLINK}",
    "JAVA_HOME": "${JAVA_HOME}",
    "HADOOP_HOME": "/usr/local/hadoop",
    "HADOOP_CONF_DIR": "/usr/local/hadoop/etc/hadoop",
    "PYSPARK_PYTHON": "${PYSPARK_PYTHON}",
    "PYTHONPATH": "${SPARK_SYMLINK}/python:${py4j_zip}"
  }
}
EOF
  chmod 0644 "${kernels_dir}/kernel.json"
  # Provide a launcher that activates the venv
  cat >/usr/local/bin/jupyter-pyspark <<EOF
#!/usr/bin/env bash
source "${JUPYTER_VENV_DIR}/bin/activate"
export SPARK_HOME="${SPARK_SYMLINK}"
export JAVA_HOME="${JAVA_HOME}"
export HADOOP_HOME="/usr/local/hadoop"
export HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop"
export PYSPARK_PYTHON="${PYSPARK_PYTHON}"
exec jupyter-lab --ip="${JUPYTER_BIND_IP}" --port="${JUPYTER_PORT}" --no-browser "$@"
EOF
  chmod +x /usr/local/bin/jupyter-pyspark

  if [[ "${JUPYTER_SYSTEMD}" == "true" ]]; then
    local run_user="${SUDO_USER:-$USER}"
    local work_dir; work_dir=$(eval echo "~${run_user}")
    cat >/etc/systemd/system/jupyter-lab.service <<EOF
[Unit]
Description=JupyterLab (PySpark on YARN)
After=network.target

[Service]
Type=simple
User=${run_user}
WorkingDirectory=${work_dir}
Environment="PATH=${JUPYTER_VENV_DIR}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin"
ExecStart=/usr/local/bin/jupyter-pyspark
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF
    systemctl daemon-reload
    systemctl enable jupyter-lab
  fi
  deactivate || true
}

discover_workers() {
  if [[ -n "${WORKERS_STR}" ]]; then
    # shellcheck disable=SC2206
    WORKERS=(${WORKERS_STR})
    return
  fi
  echo "Auto-discovering workers in /etc/hosts (workerN names)..."
  mapfile -t WORKERS < <(awk '/^[[:space:]]*[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+/ && $2 ~ /^worker[0-9]+$/ {print $2}' /etc/hosts | sort -V)
}

copy_to_workers() {
  if [[ ${#WORKERS[@]:-0} -eq 0 ]]; then echo "No workers to sync."; return; fi
  local extracted="${INSTALL_DIR}/spark-${SPARK_VERSION}-bin-${HADOOP_PROFILE}"
  echo "Copying Spark ${SPARK_VERSION} to workers: ${WORKERS[*]}"
  for w in "${WORKERS[@]}"; do
    echo " ==> ${w}"
    ssh -o StrictHostKeyChecking=accept-new "${w}" "sudo mkdir -p '${INSTALL_DIR}'"
    rsync -azh --delete "${extracted}/" "${w}:${extracted}/"
    ssh "${w}" "sudo ln -sfn '${extracted}' '${SPARK_SYMLINK}'"
    # Drop env profile with fixed Hadoop defaults
    ssh "${w}" "bash -lc 'cat | sudo tee /etc/profile.d/spark.sh >/dev/null <<EOS
export SPARK_HOME=${SPARK_SYMLINK}
export PATH=\$SPARK_HOME/bin:\$SPARK_HOME/sbin:\$PATH
export JAVA_HOME=${JAVA_HOME}
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
export PYSPARK_PYTHON=${PYSPARK_PYTHON}
EOS
sudo chmod 0644 /etc/profile.d/spark.sh'"
    # Sync conf
    rsync -azh "${SPARK_SYMLINK}/conf/" "${w}:${SPARK_SYMLINK}/conf/"
  done
}

post_notes() {
  echo
  echo "Done. Reload your shell or run: source /etc/profile.d/spark.sh"
  echo "Sanity checks:"
  echo "  spark-shell --master yarn -e 'println(sc.range(1, 1000).sum)'"
  echo "  pyspark --master yarn -c 'print(sc.parallelize(range(10)).sum())'"
  echo
  echo "Start JupyterLab (token shown):"
  echo "  jupyter-pyspark"
  echo "Pick kernel: PySpark (YARN)"
}

main() {
  require_root
  install_prereqs
  detect_java_home
  download_spark
  write_profile
  configure_spark
  maybe_create_hdfs_dirs
  setup_jupyter_env
  #discover_workers
  #copy_to_workers
  post_notes
}

main "$@"